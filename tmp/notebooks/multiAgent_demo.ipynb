{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autogen' has no attribute 'UserProxyAgent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m\n\u001b[0;32m     50\u001b[0m config_list_3 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m     {\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Let's choose the Mixtral 8x7B model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     }\n\u001b[0;32m     61\u001b[0m ]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# assistant = autogen.AssistantAgent(name=\"assistant\", llm_config=llm_config)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m \u001b[43mautogen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserProxyAgent\u001b[49m(\n\u001b[0;32m     67\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m     system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA human admin.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     69\u001b[0m     code_execution_config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_n_messages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupchat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_docker\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m     },  \u001b[38;5;66;03m# Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     human_input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTERMINATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     76\u001b[0m calc_1 \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mAssistantAgent(\n\u001b[0;32m     77\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculator 1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculator. You propose answer and avoid repeat answers.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m     llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list_1},\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     81\u001b[0m calc_2 \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mAssistantAgent(\n\u001b[0;32m     82\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculator 2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     83\u001b[0m     system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculator. You propose answer and avoid repeat answers.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     84\u001b[0m     llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list_2},\n\u001b[0;32m     85\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'autogen' has no attribute 'UserProxyAgent'"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import autogen\n",
    "import os\n",
    "\n",
    "os.environ['TOGETHER_AI_API_KEY']='1940cc7aab91e20ee2f4d6d971dd8abe078db62293b29214485a956e193fb532'\n",
    "\n",
    "# llm_config = {\n",
    "#     \"config_list\": [{\n",
    "#         \"model\": [\"meta-llama/Llama-3-8b-chat-hf\", \"meta-llama/Llama-2-7b-chat-hf\"], \n",
    "#         \"api_key\": os.environ[\"TOGETHER_AI_API_KEY\"], \n",
    "#         \"api_type\": \"together\",\n",
    "#         \"max_tokens\": 1000,\n",
    "#         \"stream\": False,\n",
    "#         \"temperature\": 1,\n",
    "#         \"top_p\": 0.8,\n",
    "#         \"top_k\": 50,\n",
    "#         \"repetition_penalty\": 0.5,\n",
    "#         \"presence_penalty\": 1.5,\n",
    "#         \"frequency_penalty\": 1.5,\n",
    "#         \"min_p\": 0.2,\n",
    "#     }],\n",
    "# }\n",
    "\n",
    "config_list_1 = [\n",
    "    {\n",
    "        # Let's choose the Mixtral 8x7B model\n",
    "        \"model\": \"meta-llama/Llama-3-8b-chat-hf\", \n",
    "        # Provide your Together.AI API key here or put it into the TOGETHER_API_KEY environment variable.\n",
    "        \"api_key\": os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "        # We specify the API Type as 'together' so it uses the Together.AI client class\n",
    "        \"api_type\": \"together\",\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 1000,\n",
    "    }\n",
    "]\n",
    "\n",
    "config_list_2 = [\n",
    "    {\n",
    "        # Let's choose the Mixtral 8x7B model\n",
    "        \"model\": \"meta-llama/Llama-3-8b-chat-hf\", \n",
    "        # Provide your Together.AI API key here or put it into the TOGETHER_API_KEY environment variable.\n",
    "        \"api_key\": os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "        # We specify the API Type as 'together' so it uses the Together.AI client class\n",
    "        \"api_type\": \"together\",\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 1000,\n",
    "    }\n",
    "]\n",
    "\n",
    "config_list_3 = [\n",
    "    {\n",
    "        # Let's choose the Mixtral 8x7B model\n",
    "        \"model\": \"meta-llama/Llama-3-8b-chat-hf\", \n",
    "        # Provide your Together.AI API key here or put it into the TOGETHER_API_KEY environment variable.\n",
    "        \"api_key\": os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "        # We specify the API Type as 'together' so it uses the Together.AI client class\n",
    "        \"api_type\": \"together\",\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 1000,\n",
    "    }\n",
    "]\n",
    "\n",
    "# assistant = autogen.AssistantAgent(name=\"assistant\", llm_config=llm_config)\n",
    "\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "calc_1 = autogen.AssistantAgent(\n",
    "    name=\"Calculator 1\",\n",
    "    system_message=\"Calculator. You propose answer and avoid repeat answers.\",\n",
    "    llm_config={\"config_list\": config_list_1},\n",
    ")\n",
    "calc_2 = autogen.AssistantAgent(\n",
    "    name=\"Calculator 2\",\n",
    "    system_message=\"Calculator. You propose answer and avoid repeat answers.\",\n",
    "    llm_config={\"config_list\": config_list_2},\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[calc_1, calc_2], \n",
    "    messages=[], \n",
    "    max_round=12,\n",
    "    allow_repeat_speaker=False,\n",
    ")\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    system_message='''\n",
    "        Thought manager. Store and aggregate results from two calculators. \n",
    "        Revise the results, guide the calculator not making repeat answer.\n",
    "    ''',\n",
    "    llm_config={\"config_list\": config_list_3},\n",
    ")\n",
    "\n",
    "query = '''\n",
    "Use numbers and basic arithmetic operations (+ - * /) to obtain 24. \n",
    "Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.\n",
    "Input: 4 4 6 8\n",
    "Steps:\n",
    "4 + 8 = 12 (left: 4 6 12)\n",
    "6 - 4 = 2 (left: 2 12)\n",
    "2 * 12 = 24 (left: 24)\n",
    "Answer: (6 - 4) * (4 + 8) = 24\n",
    "Input: 2 9 10 12\n",
    "Steps:\n",
    "12 * 2 = 24 (left: 9 10 24)\n",
    "10 - 9 = 1 (left: 1 24)\n",
    "24 * 1 = 24 (left: 24)\n",
    "Answer: (12 * 2) * (10 - 9) = 24\n",
    "Input: 4 9 10 13\n",
    "Steps:\n",
    "13 - 10 = 3 (left: 3 4 9)\n",
    "9 - 3 = 6 (left: 4 6)\n",
    "4 * 6 = 24 (left: 24)\n",
    "Answer: 4 * (9 - (13 - 10)) = 24\n",
    "Input: 1 4 8 8\n",
    "Steps:\n",
    "8 / 4 = 2 (left: 1 2 8)\n",
    "1 + 2 = 3 (left: 3 8)\n",
    "3 * 8 = 24 (left: 24)\n",
    "Answer: (1 + 8 / 4) * 8 = 24\n",
    "Input: 5 5 5 9\n",
    "Steps:\n",
    "5 + 5 = 10 (left: 5 9 10)\n",
    "10 + 5 = 15 (left: 9 15)\n",
    "15 + 9 = 24 (left: 24)\n",
    "Answer: ((5 + 5) + 5) + 9 = 24\n",
    "Input: {input}\n",
    "'''\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager, message=query.format(input='4 8 8 11')\n",
    ")\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
